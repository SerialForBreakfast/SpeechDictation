ADR: On-Device Transcription Strategies for Live Audio/Video on iOS

Context and Goals

We need to provide the best possible live audio/video transcription entirely on-device for an iOS application. This means maximizing transcription accuracy and quality (in English, for now) while maintaining real-time performance for live captions. All processing must occur privately on the device (no cloud services) to meet privacy and security requirements. The solution must be practical on recent Apple devices (iPhones/iPads with A-series or M-series chips) without excessive memory, battery, or performance issues.

Currently, the project may be using a baseline transcription method (possibly Apple’s built-in speech recognition or a single model). The goal is to evaluate options for adding additional transcription models or strategies to improve transcription quality and performance. Future considerations include optional support for other languages, so extensibility to multi-language transcription is a plus (English is the immediate focus). We will compare available on-device speech-to-text solutions, weighing their accuracy, speed, resource usage, and maintainability, and then recommend a path forward.

Options Considered

After research, we identified several on-device transcription approaches that could be adopted or combined:

Option 1: Use Apple’s On‑Device Speech Recognition APIs (Built-in Model)

Apple’s iOS provides a native Speech framework (SFSpeechRecognizer) that can perform speech-to-text on device. Since iOS 13, Apple has allowed on-device speech recognition for certain locales by setting requiresOnDeviceRecognition on the recognition request. Apple’s on-device engine uses the same technology as Siri & Dictation, and Apple continues to improve it (e.g. a new SpeechAnalyzer framework with an updated SpeechTranscriber model was introduced at WWDC 2025 for iOS 19+).

Pros:
	•	Seamless Integration and Privacy: Uses Apple’s own highly-optimized speech model running locally. No audio ever leaves the device, and there are no server limits or time restrictions for on-device mode. It’s built-in, so integration is straightforward (just use Apple’s API).
	•	Real-Time Performance: Apple’s on-device transcription is extremely fast and tuned for low latency. In tests, the new Apple SpeechTranscriber model (iOS 19+) transcribed a 34-minute recording in 45 seconds. Even on current devices, Apple’s engine can operate faster than real time, making it well-suited for live subtitles. For example, a 7.5-minute audio was transcribed in ~9 seconds on Apple’s system, versus 40 seconds with a large Transformer model – a clear speed advantage.
	•	Efficient Use of Resources: The heavy lifting is done by Apple’s Neural Engine and optimized code. The model is not packaged in the app – it’s provided by iOS and downloaded on first use if needed, so it does not increase app size. Memory usage is managed by the OS. This means no extra model files to maintain, and minimal impact on storage or RAM compared to embedding a custom model.
	•	Live Speech Features: The framework provides streaming/partial results out of the box. As the user speaks or video plays, it can deliver interim transcriptions in real time, and then update with more accurate final results. The new SpeechAnalyzer in iOS 19 adds even finer control, like distinguishing in-progress vs. final text with timestamps and formatting via AttributedString. Timing information (word timestamps) is available, which is important for aligning subtitles.
	•	Customization for Accuracy: Beginning with iOS 17, Apple allows customizing the language model on-device. Developers can supply a corpus of custom vocabulary or phrases to bias the transcriptions. This helps in niche domains – e.g. recognizing medical terms or product names – where the default model might misinterpret words. We can boost specific words or pronunciations to improve accuracy in our app’s context (as demonstrated in Apple’s WWDC23 session). This is a lightweight way to achieve some domain adaptation without retraining a model.
	•	Secure and Offline: As required, all processing is local. Apple’s implementation is also tested for user safety (e.g. it handles profanity, personal data, etc., in a controlled manner). There are no external dependencies or licenses to worry about.

Cons:
	•	Moderate Accuracy (vs. SOTA): While Apple’s on-device recognizer is good for general dictation, it is not the absolute state-of-the-art in accuracy. Independent benchmarks show that modern large-scale models like OpenAI’s Whisper can achieve lower error rates – e.g. Whisper Large has been reported with ~1% WER (word error rate) on English benchmarks, whereas Apple’s model had around 8% WER on the same test. In practice, users have noticed Apple’s dictation sometimes makes more mistakes or “hallucinates” words that weren’t said. This gap may be especially apparent with challenging audio (strong accents, fast speech, overlapping speakers, etc.) where Apple’s model might stumble more than a robust Transformer model.
	•	Limited Language Support: On-device recognition is only available for a subset of languages. Apple’s documentation doesn’t list all supported locales clearly, but roughly 10–22 languages are supported offline as of recent iOS versions (mainly English and other major languages). If we plan to support languages beyond this set in the future, Apple’s solution might not scale unless Apple adds those languages. In contrast, server-side Apple Speech (or other models) support many more languages, but our requirement is on-device only. So Apple’s built-in route could constrain us if, for example, we later need transcription for languages like Arabic or Korean – some are supported offline (e.g. Japanese, Spanish were added), but many languages still require server processing.
	•	Opaque Model and Updates: Apple’s speech model is a black box – we cannot modify how it works except through the limited APIs. If it consistently misrecognizes a word that we didn’t anticipate with the new customization API, we have no direct way to improve its acoustic model or base language model. We also rely on Apple’s update cycle for improvements. If a bug or deficiency is found, the fix might only come in the next iOS release (or not at all). This lack of control can be problematic for rapid iteration. By contrast, an open-source model could be updated or fine-tuned by us.
	•	No Full Control Over Behavior: The Apple engine may have its own censoring or formatting that we cannot override easily. For instance, it might star-out profanity or refuse certain content. It also might not include features like speaker diarization (the ability to label different speakers) – Apple’s API does not provide speaker separation in transcripts. If those features become important, Apple’s solution is limited (the new SpeechAnalyzer framework still focuses on single-speaker transcription).
	•	Historical Inconsistencies: Until iOS 16, Apple’s on-device dictation had some reliability issues – some users reported that offline mode would sometimes defer to online or that accuracy regressed in certain iOS versions. Apple is addressing these (for example, iOS 17 enhanced the dictation experience, and iOS 19’s SpeechTranscriber is a fresh model focused on long-form accuracy), but there is some risk that behavior can change across OS updates. We have to thoroughly test on each iOS version we support. Also, forcing offline mode when network is available required special handling before (with the requiresOnDeviceRecognition flag). We need to ensure that’s set so the app never inadvertently sends audio to Apple’s servers.

Option 2: Integrate OpenAI Whisper (Transformer Model via CoreML or Framework)

OpenAI Whisper is a family of advanced speech-to-text models (released in 2022) that achieve near state-of-the-art transcription accuracy. Whisper is an encoder-decoder Transformer trained on 680,000 hours of multilingual data. Notably, Whisper is open-source (MIT license) and can run offline. Various projects have ported Whisper to iOS-compatible formats, using frameworks like Core ML or optimized C++ libraries (e.g. whisper.cpp). We can integrate Whisper by converting a pre-trained model into a CoreML model or by using a library such as WhisperKit (a Swift package by Argmax) that wraps and optimizes Whisper for Apple devices.

Pros:
	•	Excellent Accuracy: Whisper’s accuracy in transcription is among the best available. On many benchmarks, it approaches human-level performance. For example, in one evaluation Whisper (large model) achieved ~1% WER on clean English audio, outperforming other systems by a large margin. It handles varying accents, dialects, and even some amount of background noise robustly thanks to its massive training dataset. Users have subjectively found Whisper far more reliable than Apple’s dictation – “blown away at how much better it is… It makes almost no errors”. By adopting Whisper, we would likely dramatically improve transcription quality (fewer misheard words, better handling of difficult audio) in our app. This directly serves our “best possible transcription” goal.
	•	Multilingual and Future-Proof: Even though we only need English now, Whisper was trained on many languages and can transcribe speech in 96+ languages, outputting either the original language or translating to English. The same model can do language detection and multi-language transcription automatically. This means if later we need to support Spanish, French, Japanese, etc., we could do so by utilizing Whisper’s capabilities (possibly just switching to a multilingual model variant). There’s no need for a separate model per language – a huge advantage in maintainability and consistency. Apple’s on-device system, in contrast, would require each language’s model to be provided by iOS or otherwise integrated. Whisper already has this covered in one model file.
	•	Rich Transcription Output: Whisper’s design includes a strong language model component, so it naturally produces text with proper punctuation, casing, and formatting. It can infer sentence boundaries and add commas, periods, question marks, etc., which are crucial for readability of transcripts. Apple’s API has improved (it can do auto-punctuation in dictation on iOS 16+), but Whisper’s output tends to be more coherent and properly formatted out-of-the-box. Whisper even attempts speaker diarization and timestamping internally – the full model can label segments for different speakers (albeit in a limited way) and provides time offsets for words. This could be leveraged for advanced features like highlighting the current speaker in live captions (though Whisper’s diarization is not its strongest suit).
	•	No Network or Costs: Whisper runs completely offline, aligning with privacy needs. Once integrated, there are no API costs or limitations – it’s free and open-source to use. Unlike cloud services (Google STT, Azure, etc.) that charge per minute, using Whisper on-device is a one-time cost of integration. This makes it cost-effective in the long run for potentially heavy usage. We also avoid latency introduced by network calls – everything is local, which is important for live usage (no waiting on server responses).
	•	Developer Control and Open Customization: Because it’s open source, we have full insight into the model and code. We can choose which model size to deploy (Whisper has several: tiny, base, small, medium, large) depending on the accuracy-speed tradeoff. We could even fine-tune the model on specific data (though this is non-trivial) or apply custom decoding tricks (like biasing certain words via a custom prefix or beam search adjustments). The open nature means if there’s a bug or an improvement (and indeed the community frequently updates whisper.cpp and related tools), we can adopt those quickly – we control updates, not a third-party vendor. This flexibility could be valuable as our needs evolve.
	•	Optimizations for Apple Silicon: Whisper models can be converted to Core ML and run on the Apple Neural Engine (ANE). Projects have shown a >3× speedup by offloading parts of Whisper’s inference to ANE via CoreML. For example, the Whisper encoder can run on ANE, while the decoder runs on CPU/GPU. Additionally, libraries like WhisperKit are optimized in Swift to use Metal and vector operations. Early results demonstrate that real-time transcription is feasible with smaller Whisper models on modern iPhones. This means we can likely achieve live streaming transcription with minimal lag using a model like Whisper small or base, especially on A14 Bionic or later devices. (Indeed, Argmax’s benchmarks show Apple’s latest model and Whisper-small are in a similar ballpark of speed on an M2 chip.) We also have options to quantize the model (8-bit or 16-bit quantization) to reduce memory and increase speed, with some loss in accuracy. In summary, technical optimizations exist to make Whisper run efficiently on-device, leveraging the power of Apple’s chips.
	•	Feature Parity and Extensibility: Many features that previously only server-based solutions had are available with Whisper or its ecosystem. For instance, need timestamps for each word? Whisper’s output includes time stamps. Need to transcribe long recordings? Whisper can handle long audio (it was designed for minutes or hours of audio with appropriate prompting). And if we ever needed to do something like summarize the transcribed text or run an NLP on it, having the transcription locally facilitates feeding it into other on-device models or workflows (and Apple’s new “Foundation” frameworks might even allow summary generation on-device in the future). Whisper’s transcription could thus be the first stage of a fully on-device pipeline for audio analysis.

Cons:
	•	Resource Intensive (Model Size & Memory): Whisper’s accuracy comes from large neural network models. The smallest English-only model (tiny.en) is ~75 MB on disk, while the largest (large-v2) is nearly 3 GB. The memory footprint during inference is higher – e.g. the small model (244 million parameters) uses roughly ~0.85 GB RAM and medium can use ~2+ GB RAM unoptimized. On an iPhone with ~4-6 GB RAM total, running such a model is taxing and could even be impossible without optimization. We would likely not be able to run the large model on an iPhone due to its size (and even if we somehow did, it would be very slow). So, we must use smaller Whisper models (small or base) to stay within practical memory and performance limits. These smaller models are less accurate than Whisper large (though still often on par with or better than Apple’s model). For example, Whisper small.en has about a 12.8% WER on one test (earnings call dataset) vs Apple at 14.0% – a bit better, but not the stunning 1% WER of the large model. In short, we may not reap the full accuracy benefit of Whisper unless we accept huge resource usage. We need to balance model size with device constraints.
	•	Inference Speed and Latency: Running a big Transformer in real-time is challenging. Apple’s engine (option 1) is purpose-built in C++/DSP for speed, while Whisper is a generic Transformer. Without acceleration, Whisper large might transcribe at only a fraction of real-time on a CPU. Even with CoreML optimizations, Whisper is slower than Apple’s STT for a given hardware when using comparable model sizes. As noted, Apple transcribed ~7.5 minutes of audio in 9 seconds, whereas Whisper large took ~40 seconds on the same test machine. That’s over 4× slower. If we choose a smaller Whisper model to gain speed, we trade away some accuracy. Whisper base or tiny can be very fast (base was processing 7.5 min in 2 seconds on an M2 in one test with heavy optimization), but at that point accuracy falls below Apple’s. So achieving both low latency and high accuracy with Whisper on mobile is a tightrope. We might experience a slight delay in live transcription output using Whisper-small vs Apple (perhaps a lag of a few hundred milliseconds to a second for Whisper to process chunks). This needs careful handling so that the user experience for live captions remains smooth.
	•	Battery and Thermal Impact: Whisper will likely use a lot of CPU/GPU during transcription. Continuous real-time transcription means continuously running the neural network on the device’s cores. This could cause battery drain and heat. Apple’s own implementation might be more energy-efficient since it uses dedicated hardware (ANE) more extensively and is highly optimized. We will need to optimize Whisper (use ANE via CoreML, quantize models to int8, etc.) to minimize this, but it may still be a heavier load than Apple’s solution. Users doing long live transcriptions might notice their device warming up or battery dropping faster with a pure Whisper-based approach.
	•	App Size and Download Complexity: Embedding a Whisper model in the app will increase its size significantly (potentially by hundreds of megabytes). For example, the small.en model is ~240 MB, and even when 8-bit quantized it could be ~150 MB+. Shipping that inside our IPA may not be ideal (App Store size limits, user download times, etc.). An alternative is to download the model on first launch or when needed. This is what Apple does for its models (and what Argmax’s WhisperKit does – it can download models at runtime). We’d need to implement a model download mechanism and manage the file securely on device. This adds some complexity (and requires user consent or at least good messaging if a large download is happening). However, since Apple’s on-device model also downloads on first use, users are somewhat accustomed to a one-time download for speech – we could piggyback on that expectation. We might also offer different model sizes as downloadable “packs” (e.g. a “High Accuracy Pack – 300 MB”). In any case, dealing with model files (download, updates, storage) is an extra engineering task that using Apple’s built-in solution avoids.
	•	No simple API for customization: Unlike Apple’s API which now lets us inject a list of custom phrases at runtime, Whisper doesn’t have an equivalent easy mechanism. We cannot fine-tune the model on-device (that would be too slow and require lots of data). We also cannot easily bias its decoding without modifying the decoding algorithm. There are some possible approaches (for instance, one could edit the beam search to add a bias towards certain words, or prepend a hint in the audio by synthesizing a prompt – but these are hacky/advanced techniques and not officially supported). Essentially, if Whisper mishears a particular word consistently, we cannot correct that except by changing the model or building a post-processing dictionary to fix its output text. This is a disadvantage for handling proper nouns or unique vocabulary specific to our app’s domain. Apple’s on-device recognizer might actually handle those better if we utilize the new customization API with a list of terms.
	•	Streaming Implementation Complexity: Whisper is inherently an offline batch model – it takes an audio segment and outputs text for that segment. To use it for live streaming audio, we need to chunk the audio (e.g. in 5-second windows), run the model on each chunk (possibly with overlap between chunks for context), and stitch together a continuous transcript. This is doable (and libraries like WhisperKit provide support for streaming mode), but it’s more complex than using Apple’s streaming API, which handles all that internally. We have to manage threading (the model inference can run asynchronously), handle partial results (perhaps we get a transcript every few seconds rather than truly word-by-word). There may also be a noticeable boundary effect if the audio is segmented (Whisper might not have context from earlier segments unless we feed it as a rolling context, which is possible using its token streaming mode, but that requires careful implementation). In summary, to achieve a smooth live transcription experience with Whisper, more engineering effort is required. We might leverage existing open-source solutions (WhisperKit’s streaming classes or whisper.cpp streaming examples), but we should plan to invest time in this.

Option 3: Other Offline Speech Recognition Models (Vosk/Kaldi, Mozilla DeepSpeech, etc.)

There are other open-source speech-to-text engines that can run on-device, which are generally smaller and less resource-hungry than Whisper, albeit with lower accuracy. Vosk (by Alpha Cephei) is one such toolkit that provides pre-trained offline models for 20+ languages. It’s based on the Kaldi ASR toolkit (using deep neural networks with traditional modeling). Mozilla DeepSpeech was another end-to-end model inspired by Baidu’s Deep Speech, trained on Mozilla’s open data; however, it has been discontinued and lags in accuracy. Wav2Vec 2.0 and SpeechBrain are newer academic models that could, in theory, be deployed via CoreML as well. Here we consider these “alternative” models as a category, since their pros/cons tend to be similar relative to Whisper or Apple.

Pros:
	•	Lightweight and Fast: These models are typically much smaller in size than Whisper. For instance, a Vosk English model is ~50 MB and still offers decent accuracy for general dictation. Because of their smaller size and simpler architecture, they run comfortably in real-time on mobile CPUs (even without GPU/ANE). They can be a good choice if we needed to support lower-end devices or wanted to minimize memory usage. The small footprint also means including them in the app is feasible without huge bloat, or they can be downloaded quickly.
	•	Streaming and Low Latency: Vosk and similar toolkits are designed for streaming speech recognition – they process audio frame-by-frame and can return partial results with very low latency (a few milliseconds). Vosk specifically provides a streaming API where you feed a continuous audio stream and it incrementally updates the transcription. This is great for live applications; it means we could get word-by-word (or phrase-by-phrase) updates nearly instantaneously, arguably even faster than Whisper or Apple’s API might output complete words. The user experience for live captions could be very responsive.
	•	Multi-language (via separate models): While each model is monolingual, the Vosk project provides models for more than 20 languages (including some that Apple doesn’t support offline, like Portuguese, Turkish, etc.). If we needed to add, say, Spanish transcription later, we could bundle the Spanish model (~40 MB) and load it as needed. Managing multiple models is straightforward since they’re independent files. This is somewhat less elegant than Whisper’s single multilingual model, but it still allows scaling to many languages with on-device processing.
	•	Custom Vocabulary and Grammar: Traditional ASR toolkits often allow you to bias recognition by supplying a list of expected words or even a formal grammar. For example, Kaldi/Vosk can load a custom dictionary or constrained language model to improve recognition of specific phrases (common in IVR systems). This means we could potentially inject domain knowledge (like a list of proper nouns) more directly than with Whisper. It’s not as easy as Apple’s SFSpeechRecognizer customization (which is just a list of strings), but it is possible to adapt Kaldi models or run them with custom language model states. If our app had a very fixed context (e.g. recognizing a set of commands or a limited vocabulary), these systems can be extremely accurate because you can tell them exactly what words to expect.
	•	Lower Requirements: These alternative models don’t necessarily need the latest hardware. Some can even run on a Raspberry Pi or older Android phones. So if supporting a wide range of devices (or preserving battery) is a concern, a small footprint model is gentle. Also, many are implemented in plain C++ and can run without requiring GPU or Neural Engine – making them robust across device variations.
	•	Open Source and Established: Kaldi (which Vosk builds on) is a long-established ASR toolkit in academia and industry, known for its accuracy in the pre-deep-learning era. It’s highly configurable. Vosk provides a simple wrapper and is Apache-licensed. Mozilla DeepSpeech (before discontinuation) was MPL-licensed and had an iOS port using TensorFlow Lite. So we have open licenses to work with and no cost. There is also a community knowledge base around these (for example, Kaldi models can be trained if we ever decided to, say, train a model on specialized data – though that is a complex task).

Cons:
	•	Lower Accuracy (general use): The accuracy of these smaller models generally does not match Whisper or Apple’s latest model. For instance, in one benchmark on common voice data, Whisper (base model) achieved 9.0% WER while a comparable Kaldi model was around 14% WER. In more challenging scenarios, the gap widens: modern end-to-end models handle informal speech and noise better than older hybrid models. The transformer-based “deep learning” approach in Whisper gives it an edge in dealing with diverse accents and noisy audio. By contrast, models like Vosk’s tend to falter more if the speaker talks fast, mumbles, or if there’s background noise, unless they were specifically trained for those conditions. We also saw user feedback that Apple’s (which itself isn’t SOTA) was still better than older systems – e.g., users called Google/Apple “abominable” but noted those at least run on device, implying that older offline options were even worse without cloud support. Choosing a lighter model could therefore undermine our “best possible transcription” objective; it would likely produce more errors or require more post-correction.
	•	Minimal Ongoing Improvement: Many of these alternative projects have slowed in development. Mozilla DeepSpeech has been formally discontinued (as of 2021), partly because newer models like Whisper overtook it. Kaldi is still maintained, but the focus of ASR research has shifted to transformers (and Kaldi’s recipes now even include chain models that are quite complex to deploy). Vosk is actively maintained as a wrapper, but it’s essentially running Kaldi models. The risk is that we adopt a technology that might not significantly improve over time, whereas Whisper and Apple’s models are on a clear trajectory of improvement (OpenAI might release better models, Apple is investing in new on-device models). We could find ourselves with a stagnating solution if we rely on older tech.
	•	Feature Gaps: These smaller models often output just raw text (all lower-case, no punctuation). We’d then have to add a post-processing step to punctuate and capitalize the transcript (there are tools for that, but it’s an extra pipeline). Whisper and Apple do this automatically. Similarly, diarization (who spoke when) is not built into Vosk – we’d need a separate speaker recognition if we wanted that. So the total system complexity might increase if we try to match the feature set of Whisper/Apple by stacking additional modules for punctuation, diarization, etc.
	•	Per-Language Model Management: If we want to support many languages with these, we have to include/download a separate model file for each language. 50 MB × many languages can add up (though we can choose to download on demand). It’s a bit more housekeeping to manage, and switching languages at runtime means unloading one model, loading another, etc. In contrast, a single Whisper model could handle multilingual input dynamically (at cost of size). Depending on how likely we are to expand beyond English, this could be a minor or major concern.
	•	Less “end-to-end” Learning: The older Kaldi models use fixed vocabulary and statistical language models. That means they might completely fail on words they’ve never seen (OOV – out-of-vocabulary words). Whisper (and Apple’s neural model) can spell out unheard words character by character if needed, or just have a far larger vocabulary from training. So for example, a new slang term or a rare proper noun might be impossible for a closed-vocabulary model to get right, whereas Whisper might get it phonetically correct or at least closer. This limits the “best possible” accuracy especially for proper nouns or evolving language. We could mitigate by adding those words to the model’s dictionary ahead of time (Vosk does allow adding new words), but it requires foresight and isn’t as flexible as a neural end-to-end approach.

Evaluation and Comparison

To summarize the findings, here is a comparison of the key factors for each option:
	•	Accuracy: Whisper (especially larger models) provides the highest accuracy. It was the only solution that achieved ~1% WER in benchmarks, effectively matching human transcription in some cases. Even the smaller Whisper models tend to slightly outperform Apple’s on-device model on challenging tasks (e.g. Whisper small vs Apple: 12.8% vs 14.0% WER in one long-form speech test). Apple’s on-device transcription has decent accuracy for everyday dictation but makes more mistakes – roughly an order of magnitude higher error rate than Whisper large in one evaluation. Still, Apple’s new model is much improved over old versions and might be sufficient for many scenarios. Vosk/alternative models generally rank last in accuracy; they might be fine for clear, slow speech but will likely produce more errors on fast, messy real-world audio. The gap between these and Whisper is significant (Whisper’s deep learning approach learned from a huge dataset, while others are limited by smaller training sets or older methods). For “best possible” transcripts, Whisper sets the bar.
	•	Latency and Real-Time Capabilities: Apple’s solution excels in low latency. It’s able to stream results word-by-word almost instantaneously and processes audio faster-than-real-time in batch tests. This makes it ideal for live subtitles where timing is critical (e.g. showing text nearly simultaneously with speech). Whisper’s performance depends on model size – smaller models (tiny/base) can approach real-time or faster on modern devices, especially using ANE acceleration. For instance, Whisper base on an M2 chip was extremely fast (processing 7.5 min audio in 2 sec in one test), but that was an optimized scenario. On an iPhone, we expect Whisper small or base can handle live audio with a short delay (possibly on the order of 100-500ms chunks). There may be a slight trade-off: using a model large enough to be very accurate will introduce some delay or dropped frames. Vosk and similar easily run in real-time (they were designed for it), so their latency is low – often <0.1x real-time on a phone. So for pure speed, Apple and lightweight models win; Whisper is the heaviest and could be the bottleneck if not carefully optimized. We will need to consider if a minor delay is acceptable in exchange for better accuracy.
	•	Resource Usage: Apple’s STT is highly optimized for Apple hardware. It uses the Neural Engine and doesn’t burden our app with model data – the model is managed by iOS (and shared across apps). Memory overhead is also largely hidden (the OS will load/unload the model as needed). Whisper, on the other hand, will demand significant resources from our app. Model size in memory can range from ~300 MB (tiny/base) up to ~>2 GB (medium/large). We would likely stick to a model in the few-hundred-megabyte range to be safe. We can mitigate some of this by using 8-bit quantization (reducing memory nearly 2–4x with slight accuracy loss). Also, the first run on device has overhead (CoreML compiles the model for ANE), but subsequent runs are faster. Alternative models are very light (tens of MB) and use less RAM, so they have the smallest footprint by far. If the app needs to run transcription continuously for long periods, using a smaller model or Apple’s might be more battery-efficient. Whisper’s heavy compute could be noticeable in battery tests. That said, on newer iPhones and iPads, running a medium-sized model for a moderate duration is feasible – these devices are quite powerful, and frameworks like WhisperKit have shown practical performance with acceptable thermals by offloading work to ANE and GPU.
	•	Language Flexibility: If future multi-language support is a goal, Whisper is strongly advantageous. A single Whisper model can handle almost any language we throw at it, and even do speech translation (which could be a future feature – e.g. transcribe Spanish speech into English text). Apple’s on-device supports multiple English locales and a handful of other languages (around 10-20). For any language outside that list, we’d have no on-device support unless Apple adds it. For example, if we needed Arabic or Hindi transcription on-device, Apple currently does not support those offline. We would then be forced to use a different approach for those users, or fall back to server-based (which we want to avoid). With Whisper, we already have capability for those languages built-in (the trade-off being that the multilingual Whisper models are larger in size). Vosk offers a compromise: it has ~20 languages, which covers many but not all languages we might care about, and we’d handle them with separate models. Expanding beyond those would require training our own Kaldi models (a complex task). So, Whisper clearly provides the broadest language coverage “for free” – a big plus for future-proofing.
	•	Customization and Domain Adaptation: Apple’s API allows simple customization (adding domain-specific phrases, pronunciations) which can yield noticeable accuracy improvements in those areas. This is done on-device at runtime and is fairly easy to implement (just supply a list of strings with optional weighting). Vosk/Kaldi also allow custom grammars or language model biasing, which could achieve similar effects if we prepare a list of likely phrases. Whisper, however, lacks an easy plug-in way to bias results. We’d mostly rely on its general model’s strength. If our use-case involves a lot of unique jargon or proper nouns, Apple’s approach might actually catch those more reliably once we feed them in (where Whisper might phonetically spell something oddly the first time it hears it). That said, Whisper’s huge training data means it knows a lot of rare words already, but we can’t be sure for niche terms. This is a point in Apple’s favor for enterprise or domain-specific applications.
	•	Maintenance and Support: Adopting Whisper or other open-source models means we own the maintenance. We need to track upstream improvements, test model updates, and handle any integration bugs. The community is active (for Whisper especially), but it’s still on us to do QA when upgrading the model version, etc. Apple’s solution shifts that burden to Apple – when iOS updates, presumably the model might improve (like the big upgrade in iOS 19’s SpeechTranscriber). However, if Apple’s model has an issue, we have little recourse except to file a feedback and wait. With open source, we could even fix issues ourselves or switch to a different model if needed. Also, using Apple’s API ties us to the iOS version; if our app needs to support older iOS versions, not all features (like on-device mode or customization) might be available on, say, iOS 12 or 13. Using our own model via CoreML could theoretically support any device that meets the hardware requirements, regardless of iOS version (CoreML has been around since iOS 11, and we can target a broad range as long as performance is acceptable). So, there’s a flexibility vs. convenience trade-off in maintenance.
	•	User Experience: With Apple’s STT, the user doesn’t have to download anything and can use the feature out-of-the-box (assuming the language model is already on device; if not, iOS will download it in the background the first time). With Whisper integration, we might have to prompt the user to download a model or ship it in the app – either way, it’s something users might notice (either a larger app install or an in-app download step). We’ll need to manage that experience carefully. Once set up, though, the end user shouldn’t notice which engine is doing the transcription aside from differences in accuracy and latency. One possible difference: Apple’s engine may censor or alter certain outputs (for instance, it might not transcribe very explicit language verbatim due to policies). Whisper will transcribe whatever it hears (including swearing, etc.). Depending on our app’s context, one or the other behavior could be preferable – we might need to add our own filtering if we use Whisper to ensure outputs meet any content guidelines.

In summary, Whisper promises superior accuracy and multi-language reach, Apple offers speed, efficiency, and ease of use, and lightweight models offer simplicity and low overhead at the cost of accuracy. The best solution may involve combining strengths or choosing the one that aligns most with our primary goal (accuracy) while mitigating its weaknesses (speed/resource cost).

Recommendation (Decision)

After careful evaluation, our recommendation is to adopt a hybrid approach, centered on integrating OpenAI’s Whisper model on-device to maximize transcription accuracy, and using Apple’s native speech recognizer strategically to ensure real-time responsiveness. In effect, we will add Whisper as a new transcription engine in the app to handle the heavy lifting of accurate transcription, while optionally leveraging Apple’s engine for immediate interim results or for lower-end scenarios. This approach aims to provide the best of both worlds and fulfill our goals:
	1.	Integrate Whisper (CoreML) as the primary transcription model for English audio. We will start with a mid-sized model such as Whisper small.en (around 244M parameters) which offers a strong accuracy vs performance balance. This model (or a quantized version) can run on modern iPhones with acceptable speed. According to benchmarks, Whisper-small should slightly outperform Apple’s on-device model in accuracy (a few percent lower WER), which will immediately improve the quality of our transcriptions. We will use an existing framework like WhisperKit to handle the Core ML model loading and possibly streaming inference, since it’s optimized for Apple devices. The model can either be bundled or downloaded on first use; given its size (~200+ MB), we lean towards hosting it for on-demand download to keep initial app size lean. Once downloaded, transcription will be fully offline and private. By using CoreML/ANE, we expect real-time or near-real-time performance. We will thoroughly test live transcription to ensure any minor latency is within acceptable limits for user experience. If needed, we can try an even smaller model (base.en) for faster speed, or optimize the decoding parameters (e.g., using a smaller beam size or even greedy decoding for a speed boost). The goal is to have Whisper provide high-accuracy transcripts with rich text (punctuation, casing) that require minimal editing or correction.
	2.	Leverage Apple’s On-Device API for real-time feedback and fallback: We do not want to lose the ultra-low-latency feedback that Apple’s engine can provide. For live displays (e.g. captions on a video or during a conversation), we plan to use Apple’s SFSpeechRecognizer in on-device mode simultaneously to get instantaneous partial results. This can be used to display words as they are spoken, giving the user immediate feedback. Once Whisper processes the audio (perhaps a second or two later) and produces a more accurate transcription for that segment, we can update/refine the text. This two-pass approach (fast initial pass by Apple, final pass by Whisper) ensures the user sees something right away, but still gets the benefit of Whisper’s accuracy for the final record or slightly delayed captions. If this proves too complex to manage in real-time (synchronizing two outputs), an alternative is to offer a “Mode” switch: e.g., Fast Mode (Apple-only, for near-zero lag needs) and Accurate Mode (Whisper, for best quality). In Accurate Mode, we might accept a small delay or chunked subtitle approach (display text with a ~1-second delay but with high accuracy). Many users may prefer accuracy, especially for recorded media transcription where a short delay is not critical. We will monitor how well Whisper can stream on its own; if it meets our latency targets by itself, we may not need Apple’s interim results at all. But it’s a useful fallback, and also provides a backup engine in case one fails or doesn’t support a certain accent – having both could even allow ensemble behavior (cross-checking one against the other for confidence, etc., though that is an expansion idea).
	3.	Maintain Privacy and On-Device Processing: Both engines we’ll be using operate fully on-device with no audio leaving the phone, satisfying our privacy requirement. The hybrid approach does not compromise that; it simply uses two local models. We will ensure requiresOnDeviceRecognition is true for Apple’s requests to avoid any inadvertent server usage. All model files (Whisper) and processing will remain local.
	4.	Future Multi-Language Support: With this plan, we are well-positioned to add other languages when needed. For example, to add Spanish transcription on-device, we could switch to a multilingual Whisper model or include a separate Whisper model fine-tuned for Spanish. Since Whisper’s multilingual model is large (~1.5GB for medium or ~2.9GB for large), we might opt for providing a selection of languages via separate smaller models (e.g., download a Spanish small model). The WhisperKit framework and CoreML can handle loading different models as needed. Apple’s on-device support will also cover some major languages; where it does, we can use it similarly as we plan for English (quick captions). But for comprehensive language support beyond Apple’s list, Whisper gives us a clear path. In short, this decision ensures scalability: we’re not locked into only languages Apple supports. The open-source route gives us flexibility to serve a global user base in the future by leveraging community-trained models (like Whisper) for many languages.
	5.	Continuous Improvement: By integrating an open-source model, we retain the ability to swap in improvements. If OpenAI releases “Whisper v2” or another organization releases a model that outperforms Whisper, we can evaluate and adopt it. We are not solely dependent on Apple or any single vendor’s roadmap. At the same time, we’ll keep an eye on Apple’s SpeechAnalyzer progress. The Argmax benchmark suggests Apple’s new on-device model is roughly on par with Whisper base/small in both speed and accuracy. If in the coming iOS releases Apple’s model closes the accuracy gap to where it’s within a few percent of the best models, we might reconsider whether maintaining Whisper is worth it. Our architecture could then simplify to just using Apple’s (for devices on iOS 19+). However, given the current landscape and the feedback that “Whisper is far more accurate than Apple Dictation”, our primary focus is to bring that level of accuracy to our users as soon as possible.
	6.	User Impact: We will need to manage the introduction of Whisper carefully in the app. On first use of transcription, we may prompt the user to download the high-quality model (explaining it keeps data private and improves accuracy). We can also provide an option in settings to switch to a “lighter mode” if, for example, their device is older or if they prefer not to use extra storage. But by default, we will aim to use Whisper for the best accuracy. The slight increase in battery usage will be communicated if necessary (e.g. “Accuracy mode may consume more battery during long transcriptions”). Since live transcription is a resource-intensive task in general, users likely expect some battery cost. Our job is to minimize it via optimizations (ANE, quantization, etc.).

Ranking of Options: Based on the analysis, the priority order of solutions is:
	1.	OpenAI Whisper (CoreML on-device) – Chosen as the primary enhancement. This option best aligns with our goal of highest transcription quality. It leverages a state-of-the-art model that we can run privately on modern iOS devices. While it demands more resources, the trade-offs are manageable with model selection and optimization. It also offers the greatest future flexibility (multi-language, model upgrades). The accuracy improvements justify its integration effort.
	2.	Apple On-Device Speech (SFSpeechRecognizer / SpeechAnalyzer) – Used in a supporting role. This option excels in real-time performance and ease, and we will use it to complement Whisper, especially for instantaneous transcription needs. On its own, Apple’s solution is second-best in accuracy and might suffice for some use-cases, but given the user reports of errors and our aim for the best, we did not select it as the sole solution. It remains an important part of the strategy for speed and as a fallback (and could become more prominent if its accuracy improves to near parity with Whisper in the future).
	3.	Alternative Lightweight Models (Vosk/Kaldi, etc.) – Not recommended for our main use. These fulfill the on-device/offline requirement and are efficient, but their accuracy is not on par with either Whisper or Apple’s latest models. Adopting them would compromise transcript quality, which is against our primary goal. They might only be revisited if we find ourselves needing an ultra-small footprint solution for a specific scenario, but at present, they rank lowest in delivering the “best” transcription.

By implementing Whisper alongside the existing Apple capabilities, we aim to provide the best live transcription experience: highest accuracy with reasonable real-time performance, all on-device. This decision does introduce additional complexity (managing a new model and integration), but the benefit in transcription quality is substantial. We will monitor performance and user feedback closely as we roll it out. Overall, this strategy meets our requirements for privacy and practicality on recent Apple devices, and sets us up to be a leader in on-device transcription, delivering accurate live captions that rival cloud-based services – all while keeping user data secure on their own device.

Sources:
	•	Apple Developer Documentation and WWDC talks on on-device speech (iOS 13–17)
	•	User and developer observations on Apple vs. Whisper accuracy
	•	OpenAI Whisper research and open-source community benchmarks
	•	Argmax/WhisperKit benchmarks for Apple SpeechTranscriber vs. Whisper on macOS/iOS
	•	Vosk toolkit documentation (offline models, size, streaming)
	•	Transloadit Dev Blog on WhisperKit usage and model trade-offs on iOS.